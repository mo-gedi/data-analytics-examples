---
title: "DATA-0200 Lab 7 - Building our first machine learning model"
author: "Kyle Monahan"
output:
  pdf_document:
    toc: yes
  word_document:
    toc: yes
  html_document:
    toc: yes
  always_allow_html: true
---


```{r setup, include=FALSE}
#if (!require("knitr")) 
#install.packages("knitr")
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=500)

# These are knitr options, which is a package in R. Echo tells us that we will receive responses when running this notebook, caches saves the objects as we run them, and dpi sets the dots per inch or resolution of our figures.

```



## Today's workshop

Today we will learn all about the hottest new buzzword in all of data science: **machine learning**. We are actually trying to teach the machine (our computers) to learn and make predictions about our data. This section will be a whirlwind tour of the most common R packages for machine learning. 


If you know what you are doing and are looking for a specific package, you can take a look at the **CRAN Task View**:

https://cran.r-project.org/web/views/MachineLearning.html

This task view is a selection of packages to perform certain tasks. For example, if you know you want to use a *random forest algorithim*, you can use the `randomForest` package. If you want to learn about *Basyesian Additive Regression Trees (BART)*, you can use the `BayesTree` `BART` or `bartMachine` packages. The task view will give you an overview of what packages are available to perform a specific task.

>>> TASK : Look through the **CRAN Task Views**, available here: https://cran.r-project.org/web/views/ Find a that is interesting to you, and report back to the group about an interesting package you found. 

I also really like the work of Dr. McCulloch at ASU for simple to follow scripts for deriving many of these approaches. 

https://www.rob-mcculloch.org/2019_ml/webpage/index.html


## Python vs. R for machine learning 

Unlike in Python, the libraries for machine learning are much more decentralized and spread out. In the Python course, you might find yourself using the following libraries: 

1. TensorFlow (neural nets with C++ bindings & NVIDIA support)
2. scikit-learn (great general use, built on NumPy, SciPy, matplotlib)
3. Pattern (good for web scraping and light ML work)
4. Caffe (great for visual ML applications)
5. OpenCV (you can always manually create your model, which is fun)
6. Keras (TensorFLow or Theano backend, really nice viz, good docs)
7. So many more!

In R, there are a variety of packages that provide these materials, but the functions are spread across various libraries. 

## Python vs. R for data science 

I move between R and Python as project requirements demand, and I suggest you get used to doing the same. Remember, these functions are **doing the same math** and using similar approaches in the back end. If we learn it in one language, it's fairly easy to convert to a new language. 

>>> Note: Remember how our standard errors in R and Stata were different, since one used HC1 and the other used HC2 errors? This is fairly common, this is why you should always read the documentation if you're seeing weird results. Or perhaps you should read the documentation all the time, but I know it's a bit dry reading... 

## Types of machine learning 

In general, as we discussed during the lecture portion, ML is split into two main groups: 

1. Supervised machine learning (SML): Here, the learning algorithm is given labeled example inputs, where the labels indicate a desired result or output. For example, if we want to detect streets with potholes, we need a training dataset of labeled potholes in streets. SML is composed of classification (categorical output or outcome) and regression (numerical or continuous output).

2. Unsupervised machine learning (UML): No labels are provided here, and the use the underlying structure of the input data to classify and split the output. If you've used clustering in ArcMap or a GIS, you've done this. 

>> A cool example using **k-means** here: https://medium.com/@sitarzkonrad/interactive-3d-k-means-clustering-in-jupyter-1038470f687e

There is also semi-supervised learning, reinforcement learning, deep learning, etc. We will dive specifically into supervised and unsupervised learning in detail. For now, we will just provide a review of the concepts.


## Software requirements

### New packages

- **New:** `caret`, `mlbench`, `impute`, `naivebayes`, `glmnet`, `ranger`, `kernlab`, `caTools`, `randomForest`

- **Already used:** `tidyverse`

These are just a selection of packages that we will use. Caret is our workhorse package for focusing on creating models in R.


```{r, cache=F, message=F}
#Using pacman to install these packages https://www.rdocumentation.org/packages/pacman/versions/0.5.1

# I sometimes prefer packrat but this is certainly lighter to use pacman.

# The code below means, install this (pacman) if it isn't installed already

# The ! operator is boolean not AKA, if not require pacman is true (or if pacman is not installed), then intstall pacman

#if (!require("pacman")) install.packages("pacman")
#pacman::p_load(tidyverse, caret, mlbench, naivebayes, glmnet, ranger, kernlab, caTools, randomForest, rstanarm)


```

## Loading the data

For this walkthrough, we will be using two built-in datasets - the `iris` dataset and the `mtcars` dataset. You might wonder why we use those - almost **all** text books use these as an example. 

Kaggle, Stack Overflow, data science training - they all will use these datasets at some point.

### iris dataset

The iris dataset gives the  measurements of the variables sepal length and width, and petal length and width for 50 flowers and 3 species of iris. These species are **Iris setosa, versicolor and virginica**.

```{r}

# Load the built-in iris data
data(iris)

# Install and load data.table, if you get an error where it isn't found, please install it
library(data.table)

#Glance at the datatable using DT
data.table::data.table(iris)

```

We can always learn more about the data using the `?` operator.

```{r}
?iris
```


>>> TASK : Look online for examples of models created to investigate the iris dataset in R. What models do you find? What packages did they use? What website did you find it on?

### mtcars dataset

This Motor Trends Car Road Tests dataset is from the 1974 Motor Trend US Magazine, and give fuel consumption and 10 aspects of automobile design (1973 - 1974).

This is a very common data set to work with.

```{r}
data(mtcars)
data.table::data.table(mtcars) #DT is super efficient for larger data and has nice search, so check it out! It even supports client-server operation! 

```


### Looking at sonar data

We will also use the sonar dataset in order to train a classifier model. 

```{r}
library("mlbench")
data(Sonar)
data.table(Sonar)

```

### Boston housing data

Did you see something online with the Boston housing dataset? Check out how easily we can access that data.

We will work with this in the Tableau section as well. 

```{r}
library("MASS")
data(Boston)
data.table::data.table(Boston)

# Nice.
```

### Customer churn prediction models 

For students that might want to investigate data science in the financial or technology sections, customer churn is increasingly important. 

#### What is customer churn?

Customer churn can be defined as the number of customers that stopped using a service over time. 

For example, if Facebook lost ten users, out of a total of 100 users during that day, that would be a 10% churn rate. 

Customer acquisition is expensive, so minimizing churn is a common goal of private sector data science professionals. 

```{r}
#install.packages("C50")
#install.packages("modeldata")
library("C50")
library("modeldata")
data(mlc_churn)
dim(mlc_churn)
data.table(mlc_churn)

```

>>> TASK : Look online for examples of models created to investigate customer churn. What models do you find? What packages did they use? What website did you find it on?

### Towards your final project

This skill of looking up alternative approaches for a given data set, and trying to figure out why they selected their model is really important. 

## Our first ML model - unsupervised learning 

To start off, we will use unsupervised learning. This is when we don't have any data labels and our research question and outcome is suitable for a unsupervised approach. 

### Our model - k-means 

To use k-means, we need to pass in the pre-defined number of clusters (centers), and the nstart (the number of times to repeat to get a random model).

>>> TASK : There is a certain amount of assumptions required for each model. What are we assuming with k-means? What values would you select for our data, looking at the iris data?

Let's look at how k-means works graphically:

```{r}
i <- grep("Length", names(iris))
x <- iris[, i]
cl <- kmeans(x, 3, nstart = 10)
plot(x, col = cl$cluster)

```

By default, k-means is actually starting with a random distrubution of values, like below:

```{r}
set.seed(12)
init <- sample(3, nrow(x), replace = TRUE)
plot(x, col = init)
```


>> TASK : What does random seed mean? Try to look it up!

From this, it calculates the center of each color group. It will then assign each observation to the sub-group of the closest center, until all observations are assigned with a close enough distance, or a certain number of iterations. 

Graphically:

```{r}
par(mfrow = c(1, 2))
plot(x, col = init)
centres <- sapply(1:3, function(i) colMeans(x[init == i, ], ))
centres <- t(centres)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)

tmp <- dist(rbind(centres, x))
tmp <- as.matrix(tmp)[, 1:3]

ki <- apply(tmp, 1, which.min)
ki <- ki[-(1:3)]

plot(x, col = ki)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)
```

Animatedly:

https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif

We are looking for the smallest total within cluster sum of squares error (SS). This is a local error minimum here, and so this may not be a global minimum - the risks of unsupervised data! 

#### How to choose a number of clusters

You want to run the kmeans a number of times, and see how the SS is changing. This type of iteration to find values is really common. You'll see diminishing returns:

```{r}
ks <- 1:5
tot_within_ss <- sapply(ks, function(k) {
    cl <- kmeans(x, k, nstart = 10)
    cl$tot.withinss
})
plot(ks, tot_within_ss, type = "b")
```

## Hierarchical clustering 

We can also use hierarchical clustering. This finds the two nearest clusters until the clusters are merging into a single cluster. The distance between the clusters is proportional to the line length in a dendrogram. 

```{r}
d <- dist(iris[, 1:4])
hcl <- hclust(d)
hcl
plot(hcl)
```

Then we need to "cut the tree" at a certain number of clusters (lines).


```{r}
plot(hcl)
abline(h = 3.9, col = "red")
```


## PCA, etc. 

More to come on this - get excited for `prcomp()` and other functions! Enough clusters for now. 

## Our second ML model - logistic regression 

This is based on a lecture from my course, Environmental Data Visualization from last year. The code was created by my excellent TA, Allie Clifford, and modified by myself. Let's walk through it together.

```{r}
##--------------
## Running a simple logistic regression model in R
## Date created: 11/22/2018
## By: Allie Clifford
## Edited 10/23/2020 Kyle Monahan
## Version: 3.0
##
##---------------

#Uncomment line if package not installed
#install.packages(ggplot2)
#install.packages("ROCR")
#install.packages("plotROC")

library(ggplot2)
library(ROCR)
library(plotROC)


#Our data is stored in a csv file that has features as columns 
#and samples as rows
train_data <- read.csv("covtype.csv")

# Split into test and train 

#We will use only the first 500000 samples 
#as the rest will be used for testing,
#so we will save a copy in all_data
#and take the first 500000 samples in train_data

all_data <- train_data
train_data <- train_data[1:500000,]


#look at first 10 rows of data
head(train_data, n=10)
#get some descriptive stats
summary(train_data)
#get standard deviations
sapply(train_data, sd)


############################################################
#                                                          #
#   Features: Cover_Type, Elevation, Aspect, Slope,        #
#   Horizontal_Distance_To_Hydrology,                      #
#   Vertical_Distance_To_Hydrology,                        #
#   Horizontal_Distance_To_Roadways, Hillshade_9am,        #
#   Hillshade_Noon, Hillshade_3pm,                         #
#   Horizontal_Distance_To_Fire_Points,                    #
#   Wilderness_Area1, Wilderness_Area2, Wilderness_Area3,  #
#   Wilderness_Area4, Soil_Type1, Soil_Type2, Soil_Type3,  #
#   ..., Soil_Type40                                       #
#                                                          #
############################################################
#                                                          #
#  Note: Approx 40 soil types & 4 Wilderness areas         #
#                                                          #
#  Cover types are:                                        #
#  1. Spruce/fir                                           #
#  2. Lodgepole Pine                                       #
#  3. Ponderosa Pine                                       # 
#  4. Cottonwood/Willow                                    #
#  5. Aspen                                                #
#  6. Douglas-fir                                          #
#  7. Krummholz                                            #
#                                                          #   
############################################################
#                                                          #
#  For this exercise we are interested in predicting the   #
#  probability that a given site sample will contain       #
#  pine trees type 2 or type 3. Therefore we will turn     #
#  this into a binary logistic regression problem          #
#  We will convert Cover_Type into a binary where          #
#  if type == 2 or 3, we will change the value to a 1      #
#  All other types (1,4,5,6,7) will change to a 0.         #
#                                                          #
############################################################


```


```{r}
train_data$Cover_Type[train_data$Cover_Type == 1] <- 0
train_data$Cover_Type[train_data$Cover_Type == 2] <- 1
train_data$Cover_Type[train_data$Cover_Type == 3] <- 1
train_data$Cover_Type[train_data$Cover_Type == 4] <- 0
train_data$Cover_Type[train_data$Cover_Type == 5] <- 0
train_data$Cover_Type[train_data$Cover_Type == 6] <- 0
train_data$Cover_Type[train_data$Cover_Type == 7] <- 0

all_data$Cover_Type[all_data$Cover_Type == 1] <- 0
all_data$Cover_Type[all_data$Cover_Type == 2] <- 1
all_data$Cover_Type[all_data$Cover_Type == 3] <- 1
all_data$Cover_Type[all_data$Cover_Type == 4] <- 0
all_data$Cover_Type[all_data$Cover_Type == 5] <- 0
all_data$Cover_Type[all_data$Cover_Type == 6] <- 0
all_data$Cover_Type[all_data$Cover_Type == 7] <- 0



#first we will create a general linear model
#convert Cover_Type categories into a factor to 
#indicate that it should be treated as a categorical value
train_data$Cover_Type<-factor(train_data$Cover_Type)
#logistic_model <- glm(Cover_Type ~., data=train_data, family=binomial("logit"), maxit=500) #This takes some time.


logistic_model <- glm(Cover_Type ~Elevation+Slope+Horizontal_Distance_To_Hydrology+Vertical_Distance_To_Hydrology+Horizontal_Distance_To_Roadways+Hillshade_9am+Wilderness_Area1+Soil_Type3+Soil_Type4, data = train_data, family = binomial("logit"), maxit = 500)

#Output summary info of our model:
print(summary(logistic_model))
#First output is the call to the model
#Next are the deviance residuals, which is effectively a measure of model fit
#Summaries of the deviance statistic is what we can use to assess model fitness 
#Next the coeffiecients are displayed with there standard errors, z-stat (also
#known as Wald z-stat, and the associated p-values
#Everything with *** or ** is significant 
#Logistic regression coefficients give the change in the log odds of the outcome
#for a one unit increase in the predictor value

```

```{r}


###################################### 
#                                    #
# Plotting and evaluating the model  #
#                                    #
######################################



#Lets pull a subset of the data to test
#first remove the cover type column
test_data <- subset(all_data,select=-c(Cover_Type))
#grab samples not used in training
x <- length(all_data$Cover_Type)
print(x)
test <- test_data[500001:x,]
#grab answers for corresponding samples
answers <- all_data[500001:x,]

print(length(test$Elevation))
print(length(answers$Cover_Type))

#print(test)
#First calculate predictive accuracy:

results <- predict(logistic_model, new_data = test,type = 'response')
#print("length of results: ")
#print(length(results))
results <-ifelse(results >0.5,1,0)
err <- mean(results != answers$Cover_Type)
#Accuracy is a measure of the number of correctly classified labels
print(paste('Accuracy',1-err))




#Using the logistic_model object, output predicted probabilities for the
#samples in the test data
p <-predict(logistic_model, newdata=test, type="response")


#compare this with the answers reported in the answers object
pr <- ROCR::prediction(p, answers$Cover_Type)



#calculate and plot ROC: true positive rate and false positive rate
perf <- ROCR::performance(pr,measure = 'tpr',x.measure = 'fpr')
tpr <- perf@y.values[[1]]
fpr <- perf@x.values[[1]]

df <- data.frame(tpr = c(tpr), fpr=c(fpr)) #alpha=c(perf@alpha.values[[1]]))

plt<-ggplot(aes(x=fpr, y=tpr), data=df) + geom_line()
print(plt)

#calculate the AUC value for ROC
auc <- performance(pr, measure="auc")

print(auc@y.values) #This area under the curve. 

#Calculate and plot sensitivity and specificity
perf1 <-performance(pr, "sens","spec")
print(plot(perf1))

#Calculate and plot precision and recall
perf2 <-performance(pr, "prec","rec")
print(plot(perf2))


######################################################################
#                                                                    #
# Assessment: given the results, is this a good model for the data?  #
#                                                                    # 
#       What might some problems with the data set be?               #
#       Is it possible we can improve feature selection?             #
#       Do you think this model would scale to other forest sites    #
#               outside of Colorado?                                 #
#       Are there any ways to improve this model?                    #
#       Is there maybe a better algorithm to apply to this data set? #
#                                                                    #
######################################################################
```


## Our third ML model - kNN 

Our third and final ML model will be on k-nearest neighbors. How this works is by measuring the actual (Euclidian) distance between all neighbors and inferring the class of the unlabelled data by the class of it's nearest neighbors. 

Normally, as in the above code, we always split into a **test and a train dataset**. A single call to `knn` does it for us here.

```{r}
set.seed(12L)
tr <- sample(150, 50)
nw <- sample(150, 50)
library("class")
knnres <- knn(iris[tr, -5], iris[nw, -5], iris$Species[tr])
head(knnres)
```

We can compare the observed kNN predicted class with the expected known output to find accuracy.

```{r}
table(knnres, iris$Species[nw])
mean(knnres == iris$Species[nw])

```

This seems really good. But the value we chose for k is also an assumption.

Commonly, we are worried about model prediction accuracy (RMSE) here. But we also should worry about in-sample error - where we train too much on a single data set and **overfit** our model. 


More about cross-validation and other approaches in the next ML lab! 

# FUTURE WORK - Webscraping

## What we will talk about in the future - web scraping, pulling in data from APIs


The goals of this section are to discuss: 
* Batch downloads of data from the web (URL) using a *web scraping* approach.
* Organizing the data into a **tidy** data frame
* How to count how many values (observations) we have


```{r}
# Load the packages, install if needed
#install.packages("janitor")
library(readr)        # Used for reading in data
library(dplyr)        # A data management library
library(ggplot2)      # Used for graphing
library(janitor)      # Used for data cleaning
```


## *For* Loop to Download Files

The Department for Environment, Food & Rural Affairs (DEFRA) publishes annual concentrations of pollutants for local authorities in the United Kingdom.

For example, the 2010 dataset for PM 2.5 concentrations is located at https://uk-air.defra.gov.uk/datastore/pcm/popwmpm252010byUKlocalauthority.csv
The same dataset for the year 2015 is stored at https://uk-air.defra.gov.uk/datastore/pcm/popwmpm252015byUKlocalauthority.csv

A close investigation of the URLs reveals they they both follow the same format:
https://uk-air.defra.gov.uk/datastore/pcm/popwmpm25[YEAR]byUKlocalauthority.csv

Because the URLs of the each of the annual files have the same pattern we can write a *for loop* which downloads successive files, converts them into a tidy format, stacks the data and stores them in a data frame.

A data frame, as we mentioned before, is just a **virutal Excel sheet**.
For more information on data frames, see here: https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/data.frame

We can use the `DT` package to create an interactive table embedded in document which stores the data and is sortable and searchable.


## Loops in R

To do this we need to make a loop. The general syntax, or method of writing, a loop is as follows:

for (value in list_of_values) {
  do something
}

Or, in R code: 

```{r}

for (year in 2010:2015) { 
  print(paste("The year is", year)) # This will print all the years from 2010 to 2015
}

```

Applying this knowledge, let's say we want to extract the data from 2010 to 2015. Looking at the URL, we know that the URL only changes based on the date. So, we need to paste in the URL and the year to change the date. 

We then use the pipe `%>%` to pass the data frame into four functions:

* mutate()
* select()
* gather()
* bind_rows()


```{r}

library(plyr)
library(dplyr)
df <- data.frame()

for (year in 2010:2015) {
  
ap <- read_csv(paste0("https://uk-air.defra.gov.uk/datastore/pcm/popwmpm25", year,"byUKlocalauthority.csv"), skip = 2)

ap <- ap %>% mutate(year = year) %>% dplyr::select(-`LA code`)
ap <- ap %>% tidyr::gather(indicator, value, 1:3)
df <- bind_rows(df, ap)

}

df <- janitor::clean_names(df, case = "snake") %>%
      arrange(local_authority, indicator, year)

DT::datatable(df)
```


We now have the data in as a data frame, `df` with 7254 observations of four variables. 


## Grouping Variables

We can also count the number of values per year using the `group_by()` function.

```{r}
df %>%
  group_by(year) %>%
  count()
```

  
  
# For next time - Using an Application Program Interface (API)

The goals of this section are to discuss: 
* Accessing data via an **API**
* How to deal with georeferenced data
* Creating interactive maps


```{r}
#install.packages("request")
# Load the packages
library(dplyr)      # A data management library
library(tidyr)      # Used for tidying data
library(request)    # Used for HTTP GET requests
library(leaflet)    # A library for interacitve maps
```


## Using an API

The Environmental Protection Agency (EPA) makes real-time, historical, and forecasted air quality data available to developers and scientists via the AirNow API. We will use this API to investigate air quality in Massachusetts during rush hour (6 - 11 am) this morning.

**Before porceeding** go to https://docs.airnowapi.org/ to request an AirNow API account.

We will be using the `request` library to access data trhough this API. This library provides a painless interface for communicating with APIs where one does not have to worry about prasing URI strings and reading HTTP status codes, and extracting data from a JSON string.

The API that best fits our needs is the Observations by Monitoring Site API as it allows us to specify a precise geograpical area via a bounding box.
We will refer to the documentation of this API at https://docs.airnowapi.org/Data/docs to construct our query.

There are a couple crucial things to note when constructing our query:
1. The coordnates of the bounding box must come in a specific order (see documentation)
2. The start and end times and dates are in UTC, not EST (or EDT)
3. *You must use your own API key*

**Before proceeding** make sure to replace 'INSERT YOUR API KEY HERE' with your own API key!

```{r,error=TRUE}
res <- api("https://airnowapi.org/aq/data") %>%
  api_query(bbox = '-73.5,41.3,-69.9,42.8',
            startdate = '2019-10-07T11:00',
            enddate = '2019-10-07T16:00',
            parameters = 'pm25',
            datatype = 'C',
            format = 'application/json',
            api_key = 'INSERT YOUR API KEY HERE',
            verbose = 0,
            nowcastonly = 0,
            includerawconcentrations = 0) %>%
  http()
```

>>> Take a look at the response by typing res into the console or by double-clicking on it in the environment. What type is it?


## Formatting and Anlyzing the Response

The following is good practise using varius tidyverse libraries and the pipe `%>%` operator.
Here is what's happening:
1. We convert the response from a list into a dataframe
2. We generate an unique ID for each location by combinign the latitue and longitude
3. We filter our the descriptive fields we are not inrerested in
4. Then we group the data by location and calculate the mean pm25 value for each location
5. Finally we extract the geographical coordinates from the location ID we generated

```{r,error=TRUE}
pmdata <- res %>%
  ldply(data.frame) %>%
  mutate(latlon = paste(Latitude, Longitude)) %>%
  select(latlon, Value) %>%
  group_by(latlon) %>%
  summarize(value = mean(Value)) %>%
  separate("latlon", c("lat", "lon"), sep = " ", remove = TRUE, convert = TRUE)
```


## Mapping the Data

Finally we create an interactive map of our data using Leaflet - a JavaScript library for interactive maps. However, we will not be using JavaScript.
Leaflet, like many other popular JavaScript and Python libraries have a community-developed wrappers, allowing them to be used in R.

For mor information go to https://leafletjs.com/ and https://rstudio.github.io/leaflet/


Before we create the map, we will generate a yellow-red color scale to fit our pm25 values

```{r,error=TRUE}
pal <- colorNumeric(
  palette = "YlOrRd",
  domain = pmdata$value)
```


Creating a leaflet map is very similar to plotting with ggplot - it's all about layers.
First we must add a basemap, and then we can add our datapoints on top.

```{r,error=TRUE}
m = leaflet(pmdata) %>%
  addProviderTiles(providers$CartoDB.DarkMatter) %>%
  addCircles(lng = ~lon,
             lat = ~lat,
             radius = 3000,
             color = ~pal(value),
             popup = ~as.character(value),
             label = ~as.character(value),
             stroke = FALSE,
             fillOpacity = 0.5)
m
```

Feel free to play around with different basemaps:
https://rstudio.github.io/leaflet/basemaps.html
https://leaflet-extras.github.io/leaflet-providers/preview/

You can also use an API to load an external basemap or your preference. Google Maps and Mapbox are two popular options:
https://cloud.google.com/maps-platform/maps/
https://docs.mapbox.com/api/maps/


OPTIONAL: For more information on the `~` operator, check out the following:
https://www.r-bloggers.com/the-r-formula-method-the-good-parts/
https://www.r-bloggers.com/the-r-formula-method-the-bad-parts-2/


# Working with IPO data

We will start working with code published after the WeWork IPO, investigating the recent changes in IPO valuation over the past ten years. 

This original code was written by Jonathan Regenstein, and we are just building on it: https://rviews.rstudio.com/2019/10/21/ipo-exploration/?mkt_tok=eyJpIjoiWkRkbFkyTmtPVFUyTmpNeSIsInQiOiJmV3pVVjZTRkNCSXhiSGRIWFVhTlFpVnI4ekNwV1dBQ1RjREN5TTBkTXlob05LNjBEOFlvNXVsN3FhMG1Ra3F3aFJZcEZcL0lpWGYyMG5hbDNtM25HaWlCMzBUU21cL3l5RjI4dU00ZGtlcDZ5MFVKaWQxRDVSSWZDWW00ak1SRVhZIn0%3D

```{r}
#install.packages("tidyquant")
#install.packages("riingo")
#install.packages("roll")
#install.packages("tictoc")

library(dplyr)
library(tidyverse)
library(tidyquant)
library(dplyr)
library(plotly)
library(riingo)
library(roll)
library(tictoc)
```


Now we need to start downloading data. We can use the tq_exchange() function to pull the ticker values.


```{r}
nasdaq <-
  tq_exchange("NASDAQ")

amex  <- 
  tq_exchange("AMEX")

nyse <- 
  tq_exchange("NYSE")
```


Now we can bindrows() and bring that data together, thanks to the authors of tidyquant (nice!):

```{r}
company_ipo_sector <-
  nasdaq %>% 
  bind_rows(amex) %>% 
  bind_rows(nyse) %>% 
  select(symbol, company, ipo.year, sector) %>% 
  filter(!is.na(ipo.year))


company_ipo_sector %>% 
  head()
```

Then we can count all the IPO values in each sector, and plot those. 

```{r}

company_ipo_sector_data <- company_ipo_sector %>% 
  group_by(ipo.year) %>%
  count(ipo.year)

# Rename ipo.year to ipo_year
colnames(company_ipo_sector_data)[1] <- "ipo_year"
  
g <- ggplot(data=company_ipo_sector_data, aes(x = ipo_year, y = n)) +
  geom_col(color = "cornflowerblue") + 
  scale_x_continuous(breaks = scales::pretty_breaks(n = 20)) +
  theme(axis.text.x = element_text(angle = 90))

g


```

We can use `ggplotly` to gain some plotly-esque interativity, just like we will do with Tableau!

```{r}

company_ipo_sector_data2 <- company_ipo_sector %>% 
  group_by(ipo.year) %>%
  count(ipo.year) 

# Rename ipo.year to ipo_year
colnames(company_ipo_sector_data2)[1] <- "ipo_year"

ggplotly(
  ggplot(data=company_ipo_sector_data2,aes(x = ipo_year, y = n)) +
  geom_col(color = "cornflowerblue") + 
  scale_x_continuous(breaks = scales::pretty_breaks(n = 20)) +
  theme(axis.text.x = element_text(angle = 90))
)
```

I wonder how this would work out by sector. We can use `tidy` approaches to look at this: 

```{r}

company_ipo_sector %>% 
  group_by(ipo.year) %>% 
  filter(ipo.year > 2015 & !is.na(sector)) %>%
  mutate(sector = str_remove(sector, "Consumer")) %>% 
  count(sector) %>% 
  ggplot(aes(x = sector, y = n, fill = 'sector')) +
   geom_col() +
   facet_wrap(~ipo.year) +
   theme(axis.text.x = element_text(angle = 90)) +
   labs(x = "")

```

It looks like Health Care and Finance are the places to have an IPO!


```{r}
(
  company_ipo_sector %>% 
  group_by(ipo.year) %>% 
  filter(between(ipo.year, 2004, 2019) & !is.na(sector)) %>%
  mutate(sector = str_remove(sector, "Consumer")) %>% 
  count(sector) %>% 
  ggplot(aes(x = sector, y = n, fill = sector)) +
  geom_col() +
  facet_wrap(~ipo.year,  nrow = 5) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "")
  ) %>% 
  ggplotly()
```


```{r}
(company_ipo_sector %>% 
  group_by(ipo.year) %>% 
  filter(between(ipo.year, 2004, 2019) & !is.na(sector)) %>%
  mutate(sector = str_remove(sector, "Consumer")) %>% 
  count(sector) %>% 
  ggplot(aes(x = sector, y = n, fill = sector, text = paste("sector:", sector, "<br> Number of IPOs:", n, sep = " "))) +
  geom_col() +
  facet_wrap(~ipo.year,  nrow = 5) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "")
 ) %>% 
  ggplotly(tooltip = "text")
```


We can also split it out by sector rather than year:

```{r}
(
company_ipo_sector %>% 
  group_by(ipo.year) %>% 
  filter(between(ipo.year, 2004, 2019) & !is.na(sector)) %>%
  mutate(sector = str_remove(sector, "Consumer")) %>% 
  count(sector) %>%  
  ggplot(aes(x = ipo.year, y = n, fill = sector)) +
  geom_col() +
  facet_wrap(~sector) +
  theme(axis.text.x = element_text(angle = 90))
) %>% 
  ggplotly()
```




## Potential question:

Let's say you wanted to explore unsupervised machine learning in this dataset. You want to see if there are clusters of IPO valuations (using the `count` of IPOs within a year) by sector that are detectable without using the `sector` labels.

1. What unsupervised learning approach would you use? Why?
2. Interpret the results of your unsupervised learning model. How does this compare to the number of groups found by `sector`?
3. Create a graphic showing the number of clusters of IPO counts that were selected using your clustering approach. One axis should be `count` and the other should be `year`. 
4. Create another graphic showing the true `sectors`. How well did we do? Can we assess accuracy?

## Check out this book

Check out these Python notebooks: 
https://github.com/wesm/pydata-book
